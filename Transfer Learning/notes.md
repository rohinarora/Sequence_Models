Some of the recent works that employ pretrained language models include ULMFit, ELMo, GLoMo, and OpenAI transformer.

These language modeling systems conduct deep pretraining of entire models with hierarchical representations or graph-based representations. This concept is a shift from the use of unary word embeddings, which have been effectively used to address many NLP tasks over the past few years, to the use of more sophisticated and abstract representations.


ELMo can be described as “pretraining the entire model with deep contextualized representations through stacked neural layers” as opposed to just employing the word embeddings (i.e., unary representations) as initialization


No context- (sentiment analysis)
https://medium.com/dair-ai/state-of-the-art-multimodal-sentiment-classification-in-videos-1daa8a481c5a


GPT2
https://github.com/openai/gpt-2

https://openai.com/blog/better-language-models/
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
